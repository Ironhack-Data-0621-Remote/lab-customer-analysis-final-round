{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Customer Analysis Final Round\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we still keep using the marketing_customer_analysis.csv file that you can find in the files_for_lab folder.\n",
    "\n",
    "It's time to put it all together. Remember the previous rounds and follow the steps as shown in previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Problem (case study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Description.\n",
    "\n",
    "- Goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from a customer analysis of a car ensurance company.\n",
    "# Based on the given data the objective is to create a prediction model to find out if the \n",
    "# Total claim amount can be predicted by other information avalible about the cosumer and how valid this prediction is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "data = pd.read_csv('files_for_lab/csv_files/marketing_customer_analysis.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - Cleaning/Wrangling/EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change headers names.\n",
    "- Deal with NaN values.\n",
    "- Categorical Features.\n",
    "- Numerical Features.\n",
    "- Exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dfheaders(df):\n",
    "    df.rename(columns={'Customer':'id', 'EmploymentStatus':'employment_status'}, inplace=True)\n",
    "    df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    return df\n",
    "\n",
    "# using the 2 operations together only works when removing the 'df=' infront of the first satemment. Why?\n",
    "# the first operation doesnt work at all in a function without the inplace parameter. Outside of a function it does work. Why?\n",
    "\n",
    "clean_dfheaders(data)\n",
    "\n",
    "data.duplicated(subset=['id']).unique()\n",
    "# or data['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates with drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('id', inplace=True)\n",
    "\n",
    "data['effective_to_date'] = pd.to_datetime(data['effective_to_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = list(data.select_dtypes(include=[np.number]).columns.values)\n",
    "cat_col = list(data.select_dtypes(include=[np.object]).columns.values)\n",
    "\n",
    "for col in cat_col:\n",
    "    print(data[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# cleaning columns examples\n",
    "\n",
    "data['customer_lifetime_value'] = data['customer_lifetime_value'].apply(lambda x: float(str(x).replace('%', '')))\n",
    "\n",
    "def clean_gender(x):\n",
    "    if str(x).lower().startswith('m'):\n",
    "        return 'M'\n",
    "    elif str(x).lower().startswith('f'):\n",
    "        return 'F'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "data['gender'] = data['gender'].apply(clean_gender)\n",
    "\n",
    "data['state'] = data['state'].apply(lambda x: 'California' if str(x).lower().startswith('cal')\n",
    "                                                 else 'Nevada' if x == 'NV'\n",
    "                                                 else 'Arizona' if x == 'AZ'\n",
    "                                                 else x)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# replacing NaN values \n",
    "\n",
    "def clean_df(df):\n",
    "    replace_dict = {\n",
    "        'id': '',\n",
    "        'state': 'California',\n",
    "        'customer_lifetime_value': '',\n",
    "        'response': 'No',\n",
    "        'coverage': '',\n",
    "        'education': '',\n",
    "        'effective_to_date': '',\n",
    "        'employment_status': '',\n",
    "        'gender': '',\n",
    "        'income': '',\n",
    "        'location_code': '',\n",
    "        'marital_status': '',\n",
    "        'monthly_premium_auto': '',\n",
    "        'months_since_last_claim': df['months_since_last_claim'].median(),\n",
    "        'months_since_policy_inception': '',\n",
    "        'number_of_open_complaints': df['number_of_open_complaints'].median(),\n",
    "        'number_of_policies': '',\n",
    "        'policy_type': '',\n",
    "        'policy': '',\n",
    "        'renew_offer_type': '',\n",
    "        'sales_channel': '',\n",
    "        'total_claim_amount': '',\n",
    "        'vehicle_class': 'Four-Door Car',\n",
    "        'vehicle_size': 'Medsize',\n",
    "        'vehicle_type': 'A',  \n",
    "    }\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(replace_dict[column])\n",
    "        \n",
    "# replacing the value with a mode() expression, e.g. 'state': data['state'].mode() doesnt'work. why?\n",
    "# replacing NaN on categorical columns with mode or value generateted by random, set up in a way that the ratio of unique values stays the same\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_col:\n",
    "    sns.distplot(data[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_col:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(data.corr())\n",
    "\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(data.corr(), mask=mask, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# remove multicolinearity > 0.9 \n",
    "# > 0.75 check again with model metrics\n",
    "# when removing columns, keep the ones that have the highest correlation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dealing with outliers.\n",
    "- Normalization.\n",
    "- Encoding Categorical Data.\n",
    "- Splitting into train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for values <= 0 and replacing them before tranforming\n",
    "for col in num_col:\n",
    "    neg_val = len(data[data[col] < 0])\n",
    "    zero_val = len(data[data[col] == 0])\n",
    "     \n",
    "    if neg_val > 0:\n",
    "        print('Negative values in', col, ':', neg_val)\n",
    "    elif zero_val > 0:\n",
    "        print('Zeros in', col, ':', zero_val)\n",
    "    else:\n",
    "        print('Column', col, 'is ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "repl_col = ['income', 'months_since_last_claim', 'months_since_policy_inception']\n",
    "\n",
    "for col in repl_col:\n",
    "    data_t[col] = np.where(data_t[col] == 0, data_t[col].median(), data_t[col])\n",
    "    \n",
    "trans_col = ['customer_lifetime_value', 'income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception']\n",
    "\n",
    "for col in trans_col:\n",
    "    transformed_col, _ci = stats.boxcox(data_t[col])\n",
    "    data_t[col] = transformed_col\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.distplot(data_t[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "# you can decide to remove outliers after log / boxcox transformation\n",
    "## (since these might take care of some outliers for you)\n",
    "\n",
    "# make sure you are droping the outliers only after the feature selection,\n",
    "# just so you don't end up losing rows because of outliers in a column you won't use\n",
    "\n",
    "def replace_outliers(df, threshold=1.5, in_columns=['customer_lifetime_value', 'monthly_premium_auto', 'number_of_policies'], skip_columns=[], median_repl=[]):\n",
    "    for column in in_columns:\n",
    "        if column not in skip_columns:\n",
    "            upper = np.percentile(df[column],75)\n",
    "            lower = np.percentile(df[column],25)\n",
    "            iqr = upper - lower\n",
    "            upper_limit = upper + (threshold * iqr)\n",
    "            lower_limit = lower - (threshold * iqr)\n",
    "            \n",
    "            if column not in median_repl:\n",
    "                df.loc[df[column] > upper_limit, col] = upper_limit\n",
    "                df.loc[df[column] < lower_limit, col] = lower_limit\n",
    "            else:\n",
    "                df.loc[df[column] > upper_limit, col] = df[column].median()\n",
    "                df.loc[df[column] < lower_limit, col] = df[column].median()\n",
    "    return df\n",
    "\n",
    "data_t = replace_outliers(data_t, threshold=1.5, median_repl=['customer_lifetime_value', 'monthly_premium_auto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# or remove outliers with limit or median\n",
    "\n",
    "def remove_outliers(df, threshold=1.5, in_columns=df.select_dtypes(np.number).columns, skip_columns=[]):\n",
    "    for column in in_columns:\n",
    "        if column not in skip_columns:\n",
    "            upper = np.percentile(df[column],75)\n",
    "            lower = np.percentile(df[column],25)\n",
    "            iqr = upper - lower\n",
    "            upper_limit = upper + (threshold * iqr)\n",
    "            lower_limit = lower - (threshold * iqr)\n",
    "            df = df[(df[column]>lower_limit) & (df[column]<upper_limit)]\n",
    "    return df\n",
    "\n",
    "data_t = remove_outliers(data_t, threshold=1.5, skip_columns=['total_claim_amount'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y split\n",
    "t_num = list(data_t.select_dtypes(include=[np.number]).columns.values)\n",
    "t_object = list(data_t.select_dtypes(include=[np.object]).columns.values)\n",
    "\n",
    "t_drop = t_object + [t_num[7]] + ['effective_to_date']\n",
    "x_t = data_t.drop(t_drop, axis=1)\n",
    "y = data_t['total_claim_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize and Standardize\n",
    "transformer = Normalizer().fit(x_t)\n",
    "x_normalized = transformer.transform(x_t)\n",
    "data_sn = pd.DataFrame(x_normalized)\n",
    "\n",
    "transformer = StandardScaler().fit(data_sn)\n",
    "x_standardized = transformer.transform(data_sn)\n",
    "data_sn = pd.DataFrame(x_standardized)\n",
    "\n",
    "sn_col = ['customer_lifetime_value', 'income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception', 'number_of_open_complaints', 'number_of_policies']\n",
    "\n",
    "for idx, col in enumerate(sn_col):\n",
    "    data_sn.rename(columns={idx:col}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat = data_t.select_dtypes(include = [np.object])\n",
    "\n",
    "for col in t_object:\n",
    "    print(x_cat[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "x_1h = x_cat.drop(['coverage', 'education', 'vehicle_size'], axis=1)\n",
    "x_label1 = x_cat['coverage']\n",
    "x_label2 = x_cat['education']\n",
    "x_label3 = x_cat['vehicle_size']\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='error', drop='first')\n",
    "encoder.fit(x_1h)\n",
    "\n",
    "encoded = encoder.transform(x_1h).toarray()\n",
    "data_1h = pd.DataFrame(encoded)\n",
    "# to do: data_1h.columns = encoder.categories_\n",
    "\n",
    "le1 = LabelEncoder().fit(x_label1).transform(x_label1)\n",
    "le2 = LabelEncoder().fit(x_label2).transform(x_label2)\n",
    "le3 = LabelEncoder().fit(x_label3).transform(x_label3)\n",
    "\n",
    "data_le1 = pd.DataFrame(le1)\n",
    "data_le1.columns = ['coverage']\n",
    "data_le2 = pd.DataFrame(le2)\n",
    "data_le2.columns = ['education']\n",
    "data_le3 = pd.DataFrame(le3)\n",
    "data_le2.columns = ['vehicle_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([data_sn, data_1h, data_le1, data_le2, data_le3], axis=1)\n",
    "# losing column names\n",
    "\n",
    "data_x = pd.DataFrame(x)\n",
    "data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "x = data_x\n",
    "y = data_t['total_claim_amount']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 - Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 - Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- R2.\n",
    "- MSE.\n",
    "- RMSE.\n",
    "- MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions, squared=True)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"MAE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07 - Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['true'] = data_t['total_claim_amount']\n",
    "results['pred'] = predictions\n",
    "results['residual'] = results.apply(lambda x: abs(x['true'] - x['pred']), axis=1)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3Brew",
   "language": "python",
   "name": "python3brew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
